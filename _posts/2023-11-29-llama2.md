---
title: "[Paper Review] LLAMA 2: Open Foundation and Fine-Tuned Chat Models"
excerpt: "Paper review for LLAMA-2."
categories:
  - NLP
tags:
  - NLP
  - LLaMA
  - LLM
---

## Introduction

![img](/images/posts/2023-11-29-llama2/llama2_safety.png)

As described in [the previous post](https://hahminlew.github.io/nlp/llama/), LLaMA-1 emphasized open-source contributions in terms of training procedures as well as evaluations.
In this paper, Meta keeps continuing to consider open-sourcing, reproducibility, and the safety of LLMs.
I summarized key contents of this paper in this post.

## Pretraining and Evaluations

![img](/images/posts/2023-11-29-llama2/llama2_difference.png)

Meta released LLAMA-2 and LLAMA-2-CHAT, which are a family of pre-trained and fine-tuned LLMs.
The main differences between LLAMA-1 and LLAMA-2 are shown in the table; (1) changed into a new mix of data, (2) doubled context lengths and tokens (some of the tokens were 40%), (3) and used grouped-query attention (GQA).
The authors mentioned they did their best to remove personal information from the data.

For evaluations, they compared open-source base models on several benchmarks. 
Please refer to [the previous post](https://hahminlew.github.io/nlp/llama/#evaluations) to see the details.
They also compared LLAMA-2-70B with closed-source LLMs such as GPT-4 and PaLM-2.

## Fine-tuning

![img](/images/posts/2023-11-29-llama2/llama2chat_training.png)

One of their contributions in this paper is LLAMA-2-CHAT.
They employed several techniques as follows:

- **Supervised Fine-tuning (SFT)**
- **Reinforcement Learning with Human Feedback (RLHF)**
- **Reward Modeling**
- **Ghost Attention (GAtt)**

## Supervised Fine-tuning (SFT)
The authors empirically found that the existing SFT datasets lack diversity and are low quality.
Thus, they collected high-quality SFT data (a total of 27,540 annotations) and additionally annotated them through human annotators.
They also carefully analyzed the model performances through manual validation.

## Reinforcement Learning with Human Feedback (RLHF) 
RLHF is a useful model training strategy which can align the model with human preferences.
Meta focused on two metrics: (1) helpfulness and (2) safety.

## Reward Modeling
Since the above two metrics sometimes show trade-off, they utilized two seperate reward models.
They initialized these reward models from pretrained LLMs.
Here are the training procedures and objective functions to train the reward models.

|Input|Output|Objective function|
|:---|:---|:---|
|answer & corresponding prompt (including previous context)|score (helpfulness & safety, respectively)|a binary ranking loss with a margin of the preference rating (a discrete function)|

The following contexts are described about the datasets for two metrics, the training details, and results.

### Iterative Fine-tuning
They explored to find the best performative model through two major approaches: (1) Proximal Policy Optimization (PPO) (2) Rejection Sampling fine-tuning.

## GAtt Method

![img](/images/posts/2023-11-29-llama2/llama2_GAtt.png)

***
