---
title: "[Paper Review] LLAMA 2: Open Foundation and Fine-Tuned Chat Models"
excerpt: "Paper review for LLAMA-2."
categories:
  - NLP
tags:
  - NLP
  - LLaMA
  - LLM
---

## Introduction

![img](/images/posts/2023-11-29-llama2/llama2_safety.png)

As described in [the previous post](https://hahminlew.github.io/nlp/llama/), LLaMA-1 emphasized open-source contributions in terms of training procedures as well as evaluations.
In this paper, Meta keeps continuing to consider open-sourcing, reproducibility, and the safety of LLMs.
I summarized key contents of this paper in this post.

## Pretraining and Evaluations

![img](/images/posts/2023-11-29-llama2/llama2_difference.png)

They released LLAMA-2 and LLAMA-2-CHAT, which are a family of pre-trained and fine-tuned LLMs.
The main differences between LLAMA-1 and LLAMA-2 are shown in the table; (1) changed into a new mix of data, (2) doubled context lengths and tokens (some of the tokens were 40%), (3) and used grouped-query attention (GQA).
The authors mentioned they did their best to remove personal information from the data.

For evaluations, they compared open-source base models on several benchmarks. 
Please refer to [the previous post](https://hahminlew.github.io/nlp/llama/#evaluations) to see the details.
They also compared LLAMA-2-70B with closed-source LLMs such as GPT-4 and PaLM-2.

## Fine-tuning

![img](/images/posts/2023-11-29-llama2/llama2chat_training.png)

One of their contributions in this paper is LLAMA-2-CHAT.
They employed several techniques as follows:

- **Supervised Fine-tuning (SFT)**
- **Reward Modeling**
- **Reinforcement Learning with Human Feedback (RLHF)**
- **Ghost Attention (GAtt)**

## Supervised Fine-tuning (SFT)
The authors empirically found that the existing SFT datasets lack diversity and are low quality.
Thus, they collected high-quality SFT data (total 27,540 annotations) and additionally annotated them through human annotators.
They also carefully analyzed the model performances through manual validation.

## Reinforcement Learning with Human Feedback (RLHF) 



***
