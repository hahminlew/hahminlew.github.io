---
title: "[Paper Review] QLoRA: Efficient Finetuning of Quantized LLMs"
excerpt: "Paper review for QLoRA."
categories:
  - Finetuning
tags:
  - Finetuning
  - QLoRA
  - LLM
  - NLP
---

## Introduction

![img](/images/posts/2024-01-02-qlora/qlora.png)

<img src="/images/posts/2024-01-02-qlora/table1.png" width="50%"/>

As I described in [the previous post](https://hahminlew.github.io/finetuning/lora/), the efficient finetuning method of large language models (LLMs) have been treated as an important work.
In this paper, the authors presented QLoRA that acheived quantized 4-bit finetuning without sacrificing model performances.
They finetuned publicly available models (such as LLaMA) on a single GPU and named model family as **Guanaco**.
Guanaco 7B to 65B showed comparable or even better performances compared to benchmarks (GPT-4, Vicuna, and Bard) under Elo rating system while using 6GB to 41GB GPU resources.

The authors successfully designed to reduce the cost without any performace degradation through (1) **4-bit NormalFloat**, (2) **Double Quantization**, and (3) **Paged Optimizers**.
Thanks to QLoRA's efficiency, they also investigated an in-depth study on extensively finetuning over 1,000 models varing on datasets, architectures, and sizes (80M to 65B parameters).
In addition, they also analyzed chatbot performance from both human and model-based evaluation (GPT-4).

## Methods
### 4-bit NormalFloat Quantization
The NormalFloat (NF) data type builds on Quantile Quantization \[1\].

- Quantile Quantization Example (SRAM-Quantiles estimation algorithm)

|**Quantization Process**|**Dequantization Process**|
|:---|:---|
| Optimizer State: -3.1 0.1 -0.03 1.2 <br> Chunk into blocks: (-3.1 0.1) (-0.03 1.2) <br> Find block-wise absmax: 3.1 1.2 <br> Normalize with absmax: (-1.0 0.032) (-0.025 1.0) <br> Find closest 8-bit value: -1.0 0.0329 -0.0242 1.0 <br> Find corresponding index: 0 170 80 255 <br> -> Store index values|Load index values -> 0 170 80 255 <br> Lookup values: -1.0 0.0329 -0.0242 1.0 <br> Denormalize by absmax: (-1.0 0.0329) * 3.1 (-0.0242 1.0) * 1.2 <br> Dequantized optimizer states: -3.1 0.102 -0.029 1.2|

Since quantile quantization should estimate the desired quantiles, this process is computationally expensive.
Even though SRAM-Qauntiles were proposed to resolve the issue, they still showed large quantization errors for outliers.

Therefore, the authors designed an information-theoretically optimal data type by transforming input tensors to a single fixed normal distribution.
Since pretrained neural network weighs a zero-centered normal distribution, they assumed the arbitrary range \[-1, 1\].

![img](/images/posts/2024-01-02-qlora/NF4.png)

As shown in the equation (4), they first estimate (2^k + 1)~N(0, 1) quantiles, where k is a quantile quantization bit.
Then, they normalize its values in to the arbitrary range.
Finally, similar to the SRAM-Quantiles estimation algorithm, they used absolute maximum rescaling to quantize the input tensors.

### Double Quantization


### Paged Optimizers

## References
\[1\] Dettmers, Tim, et al. "8-bit optimizers via block-wise quantization." arXiv preprint arXiv:2110.02861 (2021).

***