---
title: "[Paper Review] QLoRA: Efficient Finetuning of Quantized LLMs"
excerpt: "Paper review for QLoRA."
categories:
  - Finetuning
tags:
  - Finetuning
  - QLoRA
  - LLM
  - NLP
---

## Introduction

![img](/images/posts/2024-01-02-qlora/qlora.png)

<img src="/images/posts/2024-01-02-qlora/table1.png" width="50%"/>

As I described in [the previous post](https://hahminlew.github.io/finetuning/lora/), the efficient finetuning method of large language models (LLMs) have been treated as an important work.
In this paper, the authors presented QLoRA that acheived quantized 4-bit finetuning without sacrificing model performances.
They finetuned publicly available models (such as LLaMA) on a single GPU and named model family as **Guanaco**.
Guanaco 7B to 65B showed comparable or even better performances compared to benchmarks (GPT-4, Vicuna, and Bard) under Elo rating system while using 6GB to 41GB GPU resources.

The authors successfully designed to reduce the cost without any performace degradation through (1) **4-bit NormalFloat**, (2) **Double Quantization**, and (3) **Paged Optimizers**.
Thanks to QLoRA's efficiency, they also investigated an in-depth study on extensively finetuning over 1,000 models varing on datasets, architectures, and sizes (80M to 65B parameters).
In addition, they also analyzed chatbot performance from both human and model-based evaluation (GPT-4).

## Methods
### 4-bit NormalFloat Quantization


### Double Quantization


### Paged Optimizers


***