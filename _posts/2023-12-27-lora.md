---
title: "[Paper Review] LoRA: Low-Rank Adaptation of Large Language Models"
excerpt: "Paper review for LoRA."
categories:
  - Finetuning
tags:
  - Finetuning
  - LoRA
  - LLM
  - NLP
---

## Introduction

<img src="/images/posts/2023-12-27-lora/lora.png" width="50%"/>

Finetuning all model parameters of a large language model is prohibitively expensive.
Existing works tried to mitigate costs by partially updating parameters or employing external modules that have relatively fewer parameters.
However, these approaches led to inference latency or reducing sequence length and failed to solve a trade-off between cost-effectiveness and model performance.
Thus, the authors introduce a novel method, called low-rank adaptation (LoRA).

Previous works \[1, 2\] showed that pre-trained over-parameterized models are actually a low-dimensional property.
Inspired by these findings, they assumed the finetuning process also has a low intrinsic rank.

## References
\[1\] Li, Chunyuan, et al. "Measuring the intrinsic dimension of objective landscapes." arXiv preprint arXiv:1804.08838 (2018).

\[2\] Aghajanyan, Armen, Luke Zettlemoyer, and Sonal Gupta. "Intrinsic dimensionality explains the effectiveness of language model fine-tuning." arXiv preprint arXiv:2012.13255 (2020).

***