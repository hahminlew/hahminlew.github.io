---
title: "[Paper Review] LLaMA: Open and Efficient Foundation Language Models"
excerpt: "Paper review for LLaMA."
categories:
  - NLP
tags:
  - NLP
  - LLM
  - LLaMA
---

## What are Large Languages Models (LLMs)?

Recently, Large Languages Models (LLMs) have been a big game changer in various fields, e.g., industry, social media, education, and entertainment.
Since OpenAI announced the [ChatGPT](https://chat.openai.com/), big techs have tried to develop their own LLMs; meanwhile, startups have deeply searched for a way to employ their API for a specific service.
LLMs are trained on massive amounts of text data for billions of parameters to achieve generalized language understanding and generation, thus, their training costs are super expensive.
LLMs mainly consist of transformers and are trained in a self-driven or semi-supervised manner.
Through the series of posts, I will start my journey to technically understand recent LLMs (most of them will be from big techs) or NLP. 

## LLaMA

[LLaMA](https://arxiv.org/pdf/2302.13971.pdf) is a model released by Meta at 2023.
According to the paper [Training Compute-Optimal LLMs](https://arxiv.org/abs/2203.15556) (2022), Hoffmann et al. explored optimal model sizes according to the amounts of the dataset at a training level.
However, LLaMA pointed out that inference budgets are also critical from a service-wise perspective, since their work only considered train compute budgets.


***