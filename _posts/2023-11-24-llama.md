---
title: "[Paper Review] LLaMA: Open and Efficient Foundation Language Models"
excerpt: "Paper review for LLaMA-1."
categories:
  - NLP
tags:
  - NLP
  - LLM
  - LLaMA
---

## What are Large Languages Models (LLMs)?

Recently, **Large Languages Models (LLMs)** have been a big game changer in various fields, e.g., industry, social media, education, and entertainment.
Since OpenAI announced the [ChatGPT](https://chat.openai.com/), big techs have tried to develop their own LLMs; meanwhile, startups have deeply searched for a way to employ their API for a specific service.

LLMs are trained on massive amounts of text data for billions of parameters to achieve generalized language understanding and generation, thus, their training costs are super expensive.
LLMs mainly consist of transformers and are trained in a self-driven or semi-supervised manner.
Through the series of posts, I will start my journey to technically understand recent LLMs (most of them will be from big techs) or NLP. 

## LLaMA

[LLaMA](https://arxiv.org/pdf/2302.13971.pdf) is a model released by Meta at 2023.
LLaMA successfully achieved competitive performances compared to the existing LLMs such as GPT-3, Chinchilla, and PaLM with smaller parameters ranging from 7B to 65B.
There are several advantages to using LLaMA: (1) The model can be run on a single GPU. (2) The model is trained by only publicly available benchmarks that can lead to high-compatibility of open-source research.
Therefore, LLaMA is now widely employed in most language model applications on open-source communities, like Hugging Face or Github.

## Inference Budgets are Critical.
[Hoffmann et al., 2022](https://arxiv.org/abs/2203.15556) \[1\] explored optimal model sizes according to the amounts of the dataset at a training level.
However, LLaMA pointed out that inference budgets are also critical from a service-wise perspective, since \[1\] only considered train compute budgets.
<!-- In this paper, the authors proposed the training method as well as the model architecture. -->

## References
\[1\] Hoffmann, Jordan, et al. "Training compute-optimal large language models." arXiv preprint arXiv:2203.15556 (2022).

***